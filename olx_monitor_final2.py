# olx_monitor_final_v2.py
"""
OLX Monitor ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è:
  - –≥–æ—Ä–æ–¥: –ê–∫—Ç–∞—É (aktau_5633)
  - –∫–∞—Ç–µ–≥–æ—Ä–∏—è: –≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞ -> –ò–≥—Ä—ã –∏ –∏–≥—Ä–æ–≤—ã–µ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏
–ö–æ–º–∞–Ω–¥—ã –≤ –±–æ—Ç–µ: /start /stop /status
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—ë–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ seen_links.json, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å.
"""
import os
import json
import re
import asyncio
from datetime import datetime
from typing import List, Tuple, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes

# ========== –ù–ê–°–¢–†–û–ô–ö–ò (–Ω–µ –º–µ–Ω—è–π, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ) ==========
BOT_TOKEN = "8380564038:AAGfn2ULRPSSMRZzS3nudXOZjPrleRo6xK0"
CHAT_ID = 7238085445
CHECK_INTERVAL = 60  # seconds between checks
SEEN_FILE = "seen_links.json"

# Fixed category+city from your request
CATEGORY_PATH = "elektronika/igry-i-igrovye-pristavki"
CITY_SLUG = "aktau_5633"
OLX_URL = f"https://www.olx.kz/{CATEGORY_PATH}/{CITY_SLUG}/?search%5Border%5D=created_at%3Adesc"
# =========================================================

# runtime state
monitoring = False
job_instance = None
seen_links = set()


# ---------- Helpers: load/save seen ----------
def load_seen() -> None:
    global seen_links
    if os.path.exists(SEEN_FILE):
        try:
            with open(SEEN_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, list):
                seen_links = set(data)
            else:
                seen_links = set()
            print(f"[load_seen] –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(seen_links)} —Å—Å—ã–ª–æ–∫ –∏–∑ {SEEN_FILE}")
        except Exception as e:
            print("[load_seen] –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ seen file:", e)
            seen_links = set()
    else:
        seen_links = set()
        print("[load_seen] —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–Ω—ë–º —Å –ø—É—Å—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞.")


def save_seen() -> None:
    try:
        with open(SEEN_FILE, "w", encoding="utf-8") as f:
            json.dump(list(seen_links), f, ensure_ascii=False, indent=2)
        # print("[save_seen] —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ", len(seen_links))
    except Exception as e:
        print("[save_seen] –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è seen file:", e)


# ---------- Fetch & parse OLX page ----------
async def fetch_page_text(session: aiohttp.ClientSession, url: str) -> Optional[str]:
    try:
        async with session.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=25) as resp:
            if resp.status != 200:
                print(f"[fetch_page_text] HTTP {resp.status} for {url}")
                return None
            return await resp.text()
    except asyncio.CancelledError:
        raise
    except Exception as e:
        print("[fetch_page_text] –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞:", e)
        return None


def normalize_link(raw: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É ‚Äî —É–±—Ä–∞—Ç—å —è–∫–æ—Ä—è –∏ utm-–º–µ—Ç–∫–∏"""
    link = raw.split("#")[0]
    link = re.sub(r"\?.*$", "", link)
    return link


def extract_ad_id_from_link(link: str) -> str:
    """
    –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π id –∏–∑ —Å—Å—ã–ª–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å), –∏–Ω–∞—á–µ –≤–µ—Ä–Ω—É—Ç—å —Å–∞–º link.
    OLX —Å—Å—ã–ª–∫–∏ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç '-IDxxxxx' –∏–ª–∏ –ø–æ—Ö–æ–∂–∏–µ.
    """
    m = re.search(r'-(ID[a-zA-Z0-9_-]+)\.html', link)
    if m:
        return m.group(1)
    # –∏–Ω–æ–≥–¥–∞ id —Ç–æ–ª—å–∫–æ –∫–∞–∫ —á–∏—Å–ª–æ/—Å–ª–æ–≤–æ –≤ –∫–æ–Ω—Ü–µ
    m2 = re.search(r'/([^/]+)\.html$', link)
    if m2:
        return m2.group(1)
    return link


async def parse_links_from_html(html: str) -> List[Tuple[str, Optional[int]]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (link, utime) –≥–¥–µ utime ‚Äî unix timestamp (–µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω), –∏–Ω–∞—á–µ None.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Å—Ç–∞—Ä–∞–µ—Ç—Å—è –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã OLX.
    """
    soup = BeautifulSoup(html, "html.parser")

    results: List[Tuple[str, Optional[int]]] = []

    # 1) –ù–æ–≤—ã–π OLX: –∫–∞—Ä—Ç–æ—á–∫–∏ —Å data-cy="l-card"
    cards = soup.select('div[data-cy="l-card"]')
    if cards:
        for card in cards:
            # find link inside
            a = card.select_one("a[href*='/d/obyavlenie/'], a[href*='/obyavlenie/']")
            if not a:
                continue
            href = a.get("href")
            if not href:
                continue
            if href.startswith("/"):
                href = "https://www.olx.kz" + href
            href = normalize_link(href)

            # time
            utime = None
            # OLX –∏–Ω–æ–≥–¥–∞ –∫–ª–∞–¥—ë—Ç data-utime –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É
            if card.has_attr("data-utime"):
                try:
                    utime = int(card["data-utime"])
                except:
                    utime = None
            else:
                # –∏–Ω–æ–≥–¥–∞ –µ—Å—Ç—å <span data-testid="ad-date"> —Å —Ç–µ–∫—Å—Ç–æ–º, –Ω–æ –ø–∞—Ä—Å–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–µ–Ω–∞–¥—ë–∂–Ω–æ
                utime = None

            results.append((href, utime))

    else:
        # 2) fallback: –∏—Å–∫–∞—Ç—å –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å /d/obyavlenie/
        anchors = soup.find_all("a", href=re.compile(r"/d/obyavlenie/|/obyavlenie/"))
        for a in anchors:
            href = a.get("href")
            if not href:
                continue
            if href.startswith("/"):
                href = "https://www.olx.kz" + href
            href = normalize_link(href)
            results.append((href, None))

    # dedupe preserving order
    seen = set()
    ordered = []
    for link, ut in results:
        if link not in seen:
            seen.add(link)
            ordered.append((link, ut))
    return ordered


async def fetch_current_listings() -> List[Tuple[str, Optional[int]]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (link, utime) —Ç–µ–∫—É—â–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
    async with aiohttp.ClientSession() as session:
        html = await fetch_page_text(session, OLX_URL)
        if not html:
            return []
        parsed = await parse_links_from_html(html)
        return parsed


# ---------- Job: monitoring ----------
async def monitor_job(context: ContextTypes.DEFAULT_TYPE):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É, –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è.
    –†–∞–±–æ—Ç–∞–µ—Ç –ø–æ–¥ job_queue (telegram.ext).
    """
    global seen_links
    try:
        listings = await fetch_current_listings()
        if not listings:
            print(f"[{datetime.utcnow().isoformat()}] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: –Ω–µ –ø–æ–ª—É—á–∏–ª–∏ —Å–ø–∏—Å–æ–∫.")
            return

        # –û—Ç–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ (–∫–æ—Ç–æ—Ä—ã–µ –Ω–µ—Ç –≤ seen_links)
        new_items = []
        for link, utime in listings:
            key = extract_ad_id_from_link(link)
            # use key primarily, but if key equals link (no ID extracted), fall back to link
            identifier = key or link
            if identifier not in seen_links:
                new_items.append((identifier, link, utime))

        if not new_items:
            print(f"[{datetime.utcnow().isoformat()}] –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ ‚Äî –Ω–æ–≤—ã—Ö –Ω–µ—Ç.")
            return

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ (—Å–Ω–∞—á–∞–ª–∞ —Å—Ç–∞—Ä—ã–µ –∏–∑ –Ω–æ–≤–æ–≥–æ –±–ª–æ–∫–∞)
        for identifier, link, utime in reversed(new_items):
            text = f"üÜï –ù–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ:\n{link}"
            # –¥–æ–±–∞–≤–∏–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
            if utime:
                try:
                    dt = datetime.utcfromtimestamp(int(utime)).strftime("%Y-%m-%d %H:%M:%S UTC")
                    text = f"üÜï –ù–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ ({dt}):\n{link}"
                except:
                    pass
            try:
                await context.bot.send_message(chat_id=context.application.bot_data.get("target_chat", CHAT_ID), text=text)
                print("[monitor_job] Sent:", link)
            except Exception as e:
                print("[monitor_job] –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏:", e)

        # add to seen and persist
        for identifier, link, ut in new_items:
            seen_links.add(identifier)
        save_seen()

    except Exception as e:
        print("[monitor_job] unexpected error:", e)


# ---------- Command handlers ----------
async def start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global monitoring
    chat_id = update.effective_chat.id
    # save user's chat id to bot_data so job can use it
    context.application.bot_data["target_chat"] = chat_id

    if monitoring:
        await update.message.reply_text("‚ö†Ô∏è –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —É–∂–µ –∑–∞–ø—É—â–µ–Ω.")
        return

    # load previous seen set
    load_seen()

    # Initialization: mark current items as seen (do NOT send)
    await update.message.reply_text("üîÅ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞—é —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, —Å—Ç–∞—Ä—ã–µ –Ω–µ –±—É–¥—É—Ç –ø—Ä–∏—Å—ã–ª–∞—Ç—å—Å—è.")
    current = await fetch_current_listings()
    init_count = 0
    for link, utime in current:
        identifier = extract_ad_id_from_link(link) or link
        if identifier not in seen_links:
            seen_links.add(identifier)
            init_count += 1
    save_seen()
    await update.message.reply_text(f"‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–æ {init_count} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è ‚Äî –±—É–¥—É –ø—Ä–∏—Å—ã–ª–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ.")

    # schedule repeating job
    job_queue = context.application.job_queue
    # remove existing jobs if any
    for j in job_queue.get_jobs_by_name("olx_monitor"):
        j.schedule_removal()
    job_queue.run_repeating(monitor_job, interval=CHECK_INTERVAL, first=10, name="olx_monitor")

    monitoring = True
    print("[start_handler] monitoring started for chat", chat_id)


async def stop_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    global monitoring
    job_queue = context.application.job_queue
    jobs = job_queue.get_jobs_by_name("olx_monitor")
    if jobs:
        for j in jobs:
            j.schedule_removal()
    monitoring = False
    await update.message.reply_text("üõë –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
    print("[stop_handler] monitoring stopped.")


async def status_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        f"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: {'–í–ö–õ' if monitoring else '–í–´–ö–õ'}\n"
        f"–ì–æ—Ä–æ–¥: –ê–∫—Ç–∞—É\n–ö–∞—Ç–µ–≥–æ—Ä–∏—è: –ò–≥—Ä—ã –∏ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏\n–ü–∞–º—è—Ç—å —Å—Å—ã–ª–æ–∫: {len(seen_links)}"
    )


# ---------- Entrypoint ----------
def main():
    # fix for environments where event loop already running (IDLE etc.)
    try:
        import nest_asyncio
        nest_asyncio.apply()
    except Exception:
        pass

    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start_handler))
    app.add_handler(CommandHandler("stop", stop_handler))
    app.add_handler(CommandHandler("status", status_handler))

    print("‚úÖ OLX Monitor ready. Send /start in bot to initialize.")
    # Blocking call ‚Äî will manage asyncio loop internally
    app.run_polling()


if __name__ == "__main__":
    main()
